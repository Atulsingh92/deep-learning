{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858bb747",
   "metadata": {},
   "source": [
    "### From neural networks to computation graphs, and back\n",
    "\n",
    "We have seen that a feed-forward neural network is a computation graph, organized in layers. Let's consider a feedforward network with $L$ layers (numbered from $1$ to $L$) where \n",
    "- layer $l$ has $d_l$ neurons, \n",
    "- $\\sigma_l$ is the function describing layer $l$, which takes a vector $y_l = [y_{l1} \\ldots y_{ld_l}] \\in \\mathbb{R}^{d_l}$ as input and outputs a vector $z_l = [\\sigma(y_{l0}) \\ldots \\sigma(y_{ld_l}) \\ 1] \\in \\mathbb{R}^{d_l+1}$, \n",
    "- and $\\theta_l$ is the weight matrix between layers $l-1$ and $l$, $\\theta_l \\in \\mathbb{R}^{(d_{l-1}+1)\\times d_l}$.\n",
    "\n",
    "Note that the notation above which appends a \"1\" at a layer's output permits discarding the biases. Layer zero is the network's input where we append a \"1\", so $z_0=[x \\ 1]$.\n",
    "\n",
    "> Write the network's output $f(x;\\theta)$ as a composition of functions and matrix-vector products, using the $\\sigma_l$ and $\\theta_l$ notations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04208aef",
   "metadata": {},
   "source": [
    "Now let's expand this computation graph with one more layer which represents the computation of the loss $\\ell(f(x;\\theta),y)$.\n",
    "\n",
    "> What are the inputs of this global computation graph? What are the parameters associated to this last layer? Write the network's output $\\ell(f(x;\\theta),y)$ using the $\\ell$, $\\sigma_l$ and $\\theta_l$ notations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86084fdc",
   "metadata": {},
   "source": [
    "So, overall, calculating $\\nabla_\\theta \\ell(f(x;\\theta),y)$ is really calculating the gradients of a computation graph's output with respect to some of its inputs (the $\\theta$).\n",
    "\n",
    "> Can you follow the same steps we followed in class when we introduced backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125e5af",
   "metadata": {},
   "source": [
    "Let us define the intermediate variables $y_l = w_{l-1}^T x_{l-1}$ and $z_l = \\sigma_l(y_l)$. Suppose the network has $L$ layers, numbered from $1$ to $L$. Note that $z_L = f(x;\\theta)$ \n",
    "\n",
    "Now let's consider the last layer, numbered L. Write $\\nabla_{\\theta_L} \\ell(f(x;\\theta),y)$ using $\\ell'(z,y) = \\frac{\\partial \\ell}{\\partial z}(z,y)$\n",
    "\n",
    "\n",
    "\n",
    "A feed-forward neural network is a composition of functions.  \n",
    "$y_l = w_{l-1}^T x_{l-1}$  \n",
    "$z_l = \\sigma_l(y_l)$  \n",
    "\n",
    "$f_\\theta(x) = \\sigma_l(w_{l-1}^T x_{l-1})\n",
    "\n",
    "Exercise:  \n",
    "Consider a\n",
    "1. Write the empirical loss as a function of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7393ab98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
